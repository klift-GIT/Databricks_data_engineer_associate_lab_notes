{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31824586-456d-48fe-b78b-9d200100d73b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Exploring the raw data\n",
    "\n",
    "For this demo, we've generated some data and stored it in a Unity Catalog Volume. Volumes can store any type of file and can either be managed by Unity Catalog or connected to cloud storage. Spark Declarative Pipelines can automatically pick up new files and incrementally process data in a volume making your pipelines fast and efficient.\n",
    "\n",
    "Let's start by taking a look at the contents of the `raw_data` volume.\n",
    "\n",
    "**Note: this notebook is a simple Exploration Notebook, it's not part of our final Pipeline!**\n",
    "\n",
    "Having a notebook on the side to test SQL queries interactively can be very handy to accelerate exploration and build your pipelines faster!\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-engineering&org_id=2269002447546540&notebook=%2F2-sdp-python%2Fexplorations%2F01-Exploring-the-Data&demo_name=pipeline-bike&event=VIEW&path=%2F_dbdemos%2Fdata-engineering%2Fpipeline-bike%2F2-sdp-python%2Fexplorations%2F01-Exploring-the-Data&version=1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a41deb0-07f0-4175-8437-9016299bff19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "raw_data_volume = \"/Volumes/ccdp_nl_learning/bronze/raw_data/\"\n",
    "\n",
    "# Print out a list of directories in our raw_data volume and a few files from those directories\n",
    "for table in os.listdir(raw_data_volume):\n",
    "  print(table + \"/\")\n",
    "  for file in os.listdir(raw_data_volume + table)[:3]:\n",
    "    print(\"  \" + file)\n",
    "  print(\"  ...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b699297a-9420-4e35-9fa6-2be83506e750",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "It looks like we've got a few directories here with `csv` and `json` files in them. Let's start by taking a look at the maintenance logs files using the SQL `read_files` function.\n",
    "\n",
    "`read_files` supports several different file formats including `csv` and `json`. Take a look at the [Databricks documentation](https://docs.databricks.com/aws/en/sql/language-manual/functions/read_files) to see the available formats and options.\n",
    "\n",
    "Additionally, using the `STREAM` keyword `read_files` can be used in streaming tables to ingest files into Delta Lake. `read_files` leverages Auto Loader when used in a streaming table query.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2121e420-05a9-4aaa-b496-4c30c5f41615",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from read_files(\"/Volumes/ccdp_nl_learning/bronze/raw_data/maintenance_logs/*.csv\", format => \"csv\") limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9559ff2-f27c-4fbe-a5ac-173126ca16a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "These files contains the field `issue_description` which is a free text field people can use to enter in a description of the issue they ran into while using a bike. Free text fields often include character sequences that may break CSV parsers. Let's do some data exploration on this data to see if we are processing it correctly.\n",
    "\n",
    "Based on our knowledge of the system giving us this data, all the fields are required. Let's look at records where that's not the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0db02c4c-f3ff-4184-bed3-6ec3b0153c0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Yup, it looks like there's some instances where the `issue_description` fields include a newline character. We can use `multiline => true` to tell `read_files` that records may span multiple lines and see if that fixes the issue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "042df954-a221-4683-a82a-e9b3674077a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from read_files(\"/Volumes/ccdp_nl_learning/bronze/raw_data/maintenance_logs/*.csv\", format => \"csv\", multiline => true)\n",
    "where maintenance_id is null or bike_id is null or reported_time is null or resolved_time is null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d36b3a1-b835-424d-aa37-cd8632aa0232",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Let's do some quick spot checks on the `ride_logs` and `weather` files. The files in the `weather` directory are `json` files, so we need to make sure to use the `json` format option in `read_files`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f2721ae-2319-4e71-ab7e-9151e64c96b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from read_files(\"/Volumes/ccdp_nl_learning/bronze/raw_data/rides/*.csv\", format => \"csv\") limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "442df71a-29b6-47d7-a267-34e064b55897",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from read_files(\"/Volumes/ccdp_nl_learning/bronze/raw_data/weather/*.json\", format => \"json\") limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93fb3b89-4dec-44c1-b359-852dc6cab059",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from read_files(\"/Volumes/ccdp_nl_learning/bronze/raw_data/customers_cdc/*.parquet\", format => \"parquet\") limit 10"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "01-Exploring-the-Data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
